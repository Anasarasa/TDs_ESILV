{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import github_command as gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.push(file_to_transfer='TD7_Image_Captioning_CNN_RNN.ipynb',\n",
    "       message=\"model definition\",\n",
    "       repos=\"TDs_ESILV.git\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "def timing(f):\n",
    "    def wrap(*args, **kw):\n",
    "        ts = time()\n",
    "        result = f(*args, **kw)\n",
    "        te = time()\n",
    "        print('func call took: {0:.2f} sec'.format(te-ts))\n",
    "        return result\n",
    "    return wrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image captionning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Network part\n",
    "#### Get the InceptionV3 model trained on imagenet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.models import Model\n",
    "model = InceptionV3(weights='imagenet')\n",
    "# Remove the last layer (output softmax layer) from the inception v3\n",
    "model_new = Model(model.input, model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image descriptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_proj_path=\"/Users/lucbertin/Downloads/demos/\"\n",
    "flickr_folder = \"flickr30k_images/\"\n",
    "captions_file = \"results.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'image_name| comment_number| comment\\n1000092795.jpg| 0| Two young guys with shaggy hair look at their hands while hanging out in the yard .\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.check_output([\"head\", \"-n\", \"2\", folder_proj_path+flickr_folder+captions_file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>comment_number</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>Two young guys with shaggy hair look at their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Two young , White males are outside near many...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>Two men in green shirts are standing in a yard .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>A man in a blue shirt standing in a garden .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1000092795.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>Two friends enjoy time spent together .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_name  comment_number  \\\n",
       "0  1000092795.jpg               0   \n",
       "1  1000092795.jpg               1   \n",
       "2  1000092795.jpg               2   \n",
       "3  1000092795.jpg               3   \n",
       "4  1000092795.jpg               4   \n",
       "\n",
       "                                             comment  \n",
       "0   Two young guys with shaggy hair look at their...  \n",
       "1   Two young , White males are outside near many...  \n",
       "2   Two men in green shirts are standing in a yard .  \n",
       "3       A man in a blue shirt standing in a garden .  \n",
       "4            Two friends enjoy time spent together .  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "## Open descriptions dataset and append corresponding images\n",
    "df = pd.read_csv(folder_proj_path+flickr_folder+captions_file, sep='|')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(158915, 3)\n",
      "Index(['image_name', 'comment_number', 'comment'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.columns = df.columns.str.replace(' ', '')\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Appending startseq and endseq to each comment\n",
    "df['comment2'] = (\"startseq \"  +  df.comment\n",
    "                                        .str.lower()\n",
    "                                        .str.replace(r\"[^a-z0-9 ]\", \"\")\n",
    "                                        .str.split().str.join(\" \") + \" endseq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>comment_number</th>\n",
       "      <th>comment</th>\n",
       "      <th>comment2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>19999</td>\n",
       "      <td>2199200615.jpg</td>\n",
       "      <td>4   A dog runs across the grass .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           image_name                      comment_number comment comment2\n",
       "19999  2199200615.jpg   4   A dog runs across the grass .     NaN      NaN"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['comment2'].apply(lambda x: type(x)==float)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lol\n",
    "df.drop(19999, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5463"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Just take words occuring at least 10 times\n",
    "from collections import Counter\n",
    "all_words = [item for sublist in df.comment2.str.split(' ').tolist() for item in sublist]\n",
    "more_than_10_occurences = {k:val for k,val in Counter(all_words).items() if val>=10}\n",
    "\n",
    "#more_than_10_occurences\n",
    "df['comment2'] = df['comment2'].str.split(\" \").apply(lambda val: [x for x in val if more_than_10_occurences.get(x) is not None])\n",
    "len(more_than_10_occurences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SIZE=(299,299)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode(image, model_new):\n",
    "    \"\"\" Function to encode a given image into a vector of size (2048, ) using inceptionV3 \"\"\"\n",
    "    from keras.applications.inception_v3 import preprocess_input\n",
    "    import numpy as np\n",
    "    image = np.array(image) # transform img to array\n",
    "    image = np.expand_dims(image, axis=0) # add one dimension for batch (keras needs it)\n",
    "    image = preprocess_input(image) # preprocess the image for inceptionV3\n",
    "    fea_vec = model_new.predict(image) # The model beign trained already, get the encoding vector for the image after a forward pass\n",
    "    fea_vec = np.reshape(fea_vec, -1) # reshape from (1, 2048) to (2048, )\n",
    "    return fea_vec\n",
    "\n",
    "def create_dictionnaries_for_string_convertion(vocab):\n",
    "    \"\"\" Create an index to word dictionnary and a word to index one \"\"\"\n",
    "    ixtoword, wordtoix = {}, {}\n",
    "    ix = 1\n",
    "    for w in vocab:\n",
    "        wordtoix[w] = ix\n",
    "        ixtoword[ix] = w\n",
    "        ix += 1\n",
    "    return ixtoword, wordtoix\n",
    "\n",
    "ixtoword, wordtoix = create_dictionnaries_for_string_convertion(vocab=more_than_10_occurences)\n",
    "maximum_length_caption_on_all_dataset = max(df.comment2.apply(len)) # max caption length ( for homogeneity of input vectors )\n",
    "maximum_length_caption_on_all_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'startseq a man wearing a helmet red pants with white stripes going down the sides and a white and red shirt is on a small bicycle using only his hands while his legs are up in the air while another man wearing a light blue shirt with dark blue trim and black pants with red stripes going up the sides is standing nearby gesturing toward the first man and holding a small of one of the seven endseq'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(df[df[\"comment2\"].apply(lambda x: len(x)==78)].comment2[16050])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"{1: 'startseq', 2: 'two', 3: 'young', 4: 'guys', 5: 'with', 6: 'shaggy', 7: 'hair', 8: 'look', 9: 'a\",\n",
       " \"{'startseq': 1, 'two': 2, 'young': 3, 'guys': 4, 'with': 5, 'shaggy': 6, 'hair': 7, 'look': 8, 'at':\")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(ixtoword)[:100], str(wordtoix)[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### encoding sequence and padding (for creation of inputs) #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding__padding_inputs_seq(sequence, vocab, shift, max_length):\n",
    "    import numpy as np\n",
    "    encoding = list(map(vocab.get, sequence[:shift+1]))\n",
    "    encoding += [0]*(max_length-len(encoding))\n",
    "    return np.array(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\"the\", \"rabbit\", \"is\", \"in\", \"the\",\"kitchen\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([16,  0,  0,  0,  0,  0]),\n",
       " array([  16, 4380,    0,    0,    0,    0]),\n",
       " array([  16, 4380,   68,    0,    0,    0]),\n",
       " array([  16, 4380,   68,   15,    0,    0]),\n",
       " array([  16, 4380,   68,   15,   16,    0]),\n",
       " array([  16, 4380,   68,   15,   16,  105])]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[encoding__padding_inputs_seq(sequence=a, shift=i, vocab=wordtoix, max_length=6) for i in range(len(a))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### one-hot encoding based on vocab encoding for outputs (result in softmax) #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_outputs_from_seq(sequence, shift, vocab):\n",
    "    import numpy as np\n",
    "    encoding = vocab.get(sequence[shift])\n",
    "    return np.eye(len(vocab)+1)[encoding, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " array([0., 0., 0., ..., 0., 0., 0.])]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[encoding_outputs_from_seq(sequence=a, shift=i, vocab=wordtoix) for i in range(len(a))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a batch of images then the sequence of Xt inputs with respective targets Yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch_of_images(df, batch_size, model_for_encoding, folder_imgs_path, TARGET_SIZE=TARGET_SIZE, epochs=1000):\n",
    "    \"\"\" This function will only load batch_size pictures at a time (for computations)\"\"\"\n",
    "    from PIL import Image as Im\n",
    "    import pandas as pd, numpy as np\n",
    "    a=0\n",
    "    # iterate for ever (check Keras documentation)\n",
    "    while a<epochs:\n",
    "        a+=1\n",
    "        #print(\"shuffling...\")\n",
    "        ## shuffling\n",
    "        df = df.sample(n=len(df))\n",
    "        ## One epoch = One loop\n",
    "        for batch_i in range(1, len(df)//batch_size+1):\n",
    "            print(\"\\nbatch number : {} | batch_size : {}\".format(batch_i, batch_size))\n",
    "            \n",
    "            ### ======== Images encoding part ======== ####\n",
    "            # take a sample from the main dataset\n",
    "            df_sub = df.iloc[(batch_i-1)*batch_size : (batch_i)*batch_size]#.copy()#reset_index(drop=True)\n",
    "            df_sub_images, df_sub_comments = df_sub[['image_name']].copy(), df_sub[[\"image_name\", \"comment2\"]].copy()\n",
    "            #display(df_sub)\n",
    "            \n",
    "            #print(\"opening corresponding images in {}\".format(folder_imgs_path))\n",
    "            # open corresponding images in new column\n",
    "            df_sub_images['image'] = df_sub_images.image_name.apply( lambda x: Im.open(folder_imgs_path+x).resize(TARGET_SIZE))\n",
    "            \n",
    "            #print(\"encoding images using Inceptionv3 model frozen layers\")\n",
    "            # transform to array, preprocess and encode images\n",
    "            df_sub_images['image'] = df_sub_images.image.apply(lambda x: encode(x, model_for_encoding))\n",
    "            \n",
    "            \n",
    "            ### ======== Word sequence convertion to index then embedding part ======== ####\n",
    "            print(\"\\nstarting sequence convertion to index based on entire dataset vocabulary\")\n",
    "            df_sub_comments['input_sequences']  = df_sub_comments.comment2.apply(lambda x: \n",
    "                        [encoding__padding_inputs_seq(sequence=x, shift=i, vocab=wordtoix, max_length=maximum_length_caption_on_all_dataset) for i in range(len(x))])\n",
    "            df_sub_comments['output_sequences'] = df_sub_comments.comment2.apply(lambda x:\n",
    "                        [encoding_outputs_from_seq(sequence=x, shift=i, vocab=wordtoix) for i in range(len(x))])\n",
    "            df_sub_comments = df_sub_comments.apply(lambda x: x.apply(pd.Series).stack()).reset_index(drop=True).ffill()\n",
    "            \n",
    "            print(\"🔥inputs ready.\\n\")\n",
    "            #display(df_sub_images)\n",
    "            #display(df_sub_comments)\n",
    "            \n",
    "            df_sub = df_sub_images.merge(df_sub_comments, on=[\"image_name\"])\n",
    "            #display(df_sub)\n",
    "            ## problem in input1 shape: not sum of both shapes\n",
    "            # [[input1, input2],  output]\n",
    "            yield [ [np.stack(df_sub.image), np.stack(df_sub.input_sequences)], np.stack(df_sub.output_sequences) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#[[ input1, input2], output] = list(load_batch_of_images(df, batch_size=2, \n",
    "#                                                model_for_encoding=model_new, \n",
    "#                                                 folder_imgs_path=folder_proj_path+flickr_folder, epochs=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 78)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 5464)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 2048)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "### gives a vector representation of words converted into numerical indexes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#@email_sender(recipient_emails=[\"<your_email@address.com>\", \"<your_second_email@address.com>\"], sender_email=\"<grandma's_email@gmail.com>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file=\"glove/glove.6B.200d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'the -0.071549 0.093459 0.023738 -0.090339 0.056123 0.32547 -0.39796 -0.092139 0.061181 -0.1895 0.13061 0.14349 0.011479 0.38158 0.5403 -0.14088 0.24315 0.23036 -0.55339 0.048154 0.45662 3.2338 0.020199 0.049019 -0.014132 0.076017 -0.11527 0.2006 -0.077657 0.24328 0.16368 -0.34118 -0.06607 0.10152 0.038232 -0.17668 -0.88153 -0.33895 -0.035481 -0.55095 -0.016899 -0.43982 0.039004 0.40447 -0.2588 0.64594 0.26641 0.28009 -0.024625 0.63302 -0.317 0.10271 0.30886 0.097792 -0.38227 0.086552 0.047075 0.23511 -0.32127 -0.28538 0.1667 -0.0049707 -0.62714 -0.24904 0.29713 0.14379 -0.12325 -0.058178 -0.001029 -0.082126 0.36935 -0.00058442 0.34286 0.28426 -0.068599 0.65747 -0.029087 0.16184 0.073672 -0.30343 0.095733 -0.5286 -0.22898 0.064079 0.015218 0.34921 -0.4396 -0.43983 0.77515 -0.87767 -0.087504 0.39598 0.62362 -0.26211 -0.30539 -0.022964 0.30567 0.06766 0.15383 -0.11211 -0.09154 0.082562 0.16897 -0.032952 -0.28775 -0.2232 -0.090426 1.2407 -0.18244 -0.0075219 -0.041388 -0.011083 0.078186 0.38511 0.23334 0.14414 -0.0009107 -0.26388 -0.20481 0.10099 0.14076 0.28834 -0.045429 0.37247 0.13645 -0.67457 0.22786 0.12599 0.029091 0.030428 -0.13028 0.19408 0.49014 -0.39121 -0.075952 0.074731 0.18902 -0.16922 -0.26019 -0.039771 -0.24153 0.10875 0.30434 0.036009 1.4264 0.12759 -0.073811 -0.20418 0.0080016 0.15381 0.20223 0.28274 0.096206 -0.33634 0.50983 0.32625 -0.26535 0.374 -0.30388 -0.40033 -0.04291 -0.067897 -0.29332 0.10978 -0.045365 0.23222 -0.31134 -0.28983 -0.66687 0.53097 0.19461 0.3667 0.26185 -0.65187 0.10266 0.11363 -0.12953 -0.68246 -0.18751 0.1476 1.0765 -0.22908 -0.0093435 -0.20651 -0.35225 -0.2672 -0.0034307 0.25906 0.21759 0.66158 0.1218 0.19957 -0.20303 0.34474 -0.24328 0.13139 -0.0088767 0.33617 0.030591 0.25577\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.check_output([\"head\", \"-n\", \"1\", folder_proj_path+glove_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the whole embedding into memory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "embeddings_index = {} # empty dictionary\n",
    "with open(folder_proj_path+glove_file, encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "print('Loaded {} word vectors.'.format(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform integer vector representation to dense one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* embeddings_index associate a **word** to a **vector representation**\n",
    "* wordtoix associate a **word** to a **integer number**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 200-dim dense vector for each of the 5464 words in out vocabulary (word_to_idx)\n",
    "embedding_matrix = np.zeros((len(wordtoix)+1, 200)) # 200: embedding dim: the Dense representation of the word with '200 like features'\n",
    "for word, i in wordtoix.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5464, 200)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To each 5464 word is **associated a vector** \n",
    "* It's a **stack of vectors and the index i of the matrix is associated to the index of the word itself**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_length_caption_on_all_dataset\n",
    "vocab_size = len(wordtoix) + 1\n",
    "embedding_dim = 200 # dense words representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, LSTM, Input, Embedding, add\n",
    "from keras.models import Model\n",
    "\n",
    "# image feature extractor model (as 2048 vector of features)\n",
    "inputs1 = Input(shape=(2048,))\n",
    "fe1     = Dropout(0.5)(inputs1)\n",
    "fe2     = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "# partial caption sequence model (as max size of sequences (padding: 78 values in list))\n",
    "inputs2 = Input(shape=(maximum_length_caption_on_all_dataset,)) \n",
    "se1     = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], trainable=False)(inputs2)\n",
    "se2     = Dropout(0.5)(se1)\n",
    "se3     = LSTM(256)(se2)\n",
    "\n",
    "# decoder (feed forward) model\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs  = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "# merge the two input models\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 78)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 78, 200)      1092800     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 2048)         0           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 78, 200)      0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          524544      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 256)          467968      dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 256)          0           dense_4[0][0]                    \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          65792       add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 5464)         1404248     dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,555,352\n",
      "Trainable params: 2,462,552\n",
      "Non-trainable params: 1,092,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "## shuffling\n",
    "df = df.sample(n=len(df))\n",
    "## splitting\n",
    "train_size = 0.999\n",
    "df_train, df_test = df.iloc[:round(train_size*len(df))], df.iloc[round(train_size*len(df)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((158914, 4), (158755, 4), (159, 4))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare_validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func call took: 0.00 sec\n",
      "shuffling...\n",
      "starting epochs\n",
      "batch number : 1 | batch_size : 159\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>comment_number</th>\n",
       "      <th>comment</th>\n",
       "      <th>comment2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>63656</td>\n",
       "      <td>3275748918.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>A young boy dressed in a black jacket it ridi...</td>\n",
       "      <td>[startseq, a, young, boy, dressed, in, a, blac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103067</td>\n",
       "      <td>4533793084.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>Many people have gathered to protest the gove...</td>\n",
       "      <td>[startseq, many, people, have, gathered, to, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61643</td>\n",
       "      <td>3232030272.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>A woman is walking in front of a fast-food re...</td>\n",
       "      <td>[startseq, a, woman, is, walking, in, front, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23746</td>\n",
       "      <td>2300168895.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>A brown dog laying in the grass with a white ...</td>\n",
       "      <td>[startseq, a, brown, dog, laying, in, the, gra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56916</td>\n",
       "      <td>312644387.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>An older white woman makes cupcakes in her ki...</td>\n",
       "      <td>[startseq, an, older, white, woman, makes, cup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135992</td>\n",
       "      <td>55406417.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>An older guy in a garage chiseling wood .</td>\n",
       "      <td>[startseq, an, older, guy, in, a, garage, chis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39334</td>\n",
       "      <td>2683963310.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>A blond baby playing in the sand .</td>\n",
       "      <td>[startseq, a, blond, baby, playing, in, the, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83019</td>\n",
       "      <td>3713030758.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>Kid fumbles the football after taking a shot ...</td>\n",
       "      <td>[startseq, kid, the, football, after, taking, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83219</td>\n",
       "      <td>3717467586.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>A girl in a wheelchair with a red and white s...</td>\n",
       "      <td>[startseq, a, girl, in, a, wheelchair, with, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47750</td>\n",
       "      <td>2896483502.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>A white dog with black spots catches a bright...</td>\n",
       "      <td>[startseq, a, white, dog, with, black, spots, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            image_name comment_number  \\\n",
       "63656   3275748918.jpg              1   \n",
       "103067  4533793084.jpg              2   \n",
       "61643   3232030272.jpg              3   \n",
       "23746   2300168895.jpg              1   \n",
       "56916    312644387.jpg              1   \n",
       "...                ...            ...   \n",
       "135992    55406417.jpg              2   \n",
       "39334   2683963310.jpg              4   \n",
       "83019   3713030758.jpg              4   \n",
       "83219   3717467586.jpg              4   \n",
       "47750   2896483502.jpg              0   \n",
       "\n",
       "                                                  comment  \\\n",
       "63656    A young boy dressed in a black jacket it ridi...   \n",
       "103067   Many people have gathered to protest the gove...   \n",
       "61643    A woman is walking in front of a fast-food re...   \n",
       "23746    A brown dog laying in the grass with a white ...   \n",
       "56916    An older white woman makes cupcakes in her ki...   \n",
       "...                                                   ...   \n",
       "135992          An older guy in a garage chiseling wood .   \n",
       "39334                  A blond baby playing in the sand .   \n",
       "83019    Kid fumbles the football after taking a shot ...   \n",
       "83219    A girl in a wheelchair with a red and white s...   \n",
       "47750    A white dog with black spots catches a bright...   \n",
       "\n",
       "                                                 comment2  \n",
       "63656   [startseq, a, young, boy, dressed, in, a, blac...  \n",
       "103067  [startseq, many, people, have, gathered, to, p...  \n",
       "61643   [startseq, a, woman, is, walking, in, front, o...  \n",
       "23746   [startseq, a, brown, dog, laying, in, the, gra...  \n",
       "56916   [startseq, an, older, white, woman, makes, cup...  \n",
       "...                                                   ...  \n",
       "135992  [startseq, an, older, guy, in, a, garage, chis...  \n",
       "39334   [startseq, a, blond, baby, playing, in, the, s...  \n",
       "83019   [startseq, kid, the, football, after, taking, ...  \n",
       "83219   [startseq, a, girl, in, a, wheelchair, with, a...  \n",
       "47750   [startseq, a, white, dog, with, black, spots, ...  \n",
       "\n",
       "[159 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening corresponding images in /Users/lucbertin/Downloads/demos/flickr30k_images/\n",
      "encoding images using Inceptionv3 model frozen layers\n",
      "starting sequence convertion to index based on entire dataset vocabulary\n",
      "🔥inputs ready.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>image</th>\n",
       "      <th>comment2</th>\n",
       "      <th>input_sequences</th>\n",
       "      <th>output_sequences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3275748918.jpg</td>\n",
       "      <td>[0.109782934, 0.30716428, 0.4646111, 0.5252362...</td>\n",
       "      <td>startseq</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3275748918.jpg</td>\n",
       "      <td>[0.109782934, 0.30716428, 0.4646111, 0.5252362...</td>\n",
       "      <td>a</td>\n",
       "      <td>[1, 30, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3275748918.jpg</td>\n",
       "      <td>[0.109782934, 0.30716428, 0.4646111, 0.5252362...</td>\n",
       "      <td>young</td>\n",
       "      <td>[1, 30, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3275748918.jpg</td>\n",
       "      <td>[0.109782934, 0.30716428, 0.4646111, 0.5252362...</td>\n",
       "      <td>boy</td>\n",
       "      <td>[1, 30, 3, 359, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3275748918.jpg</td>\n",
       "      <td>[0.109782934, 0.30716428, 0.4646111, 0.5252362...</td>\n",
       "      <td>dressed</td>\n",
       "      <td>[1, 30, 3, 359, 165, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2233</td>\n",
       "      <td>2896483502.jpg</td>\n",
       "      <td>[0.19546174, 0.29309717, 0.04253876, 0.0373506...</td>\n",
       "      <td>frisbee</td>\n",
       "      <td>[1, 30, 19, 220, 5, 99, 222, 1189, 30, 373, 27...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2234</td>\n",
       "      <td>2896483502.jpg</td>\n",
       "      <td>[0.19546174, 0.29309717, 0.04253876, 0.0373506...</td>\n",
       "      <td>in</td>\n",
       "      <td>[1, 30, 19, 220, 5, 99, 222, 1189, 30, 373, 27...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2235</td>\n",
       "      <td>2896483502.jpg</td>\n",
       "      <td>[0.19546174, 0.29309717, 0.04253876, 0.0373506...</td>\n",
       "      <td>a</td>\n",
       "      <td>[1, 30, 19, 220, 5, 99, 222, 1189, 30, 373, 27...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2236</td>\n",
       "      <td>2896483502.jpg</td>\n",
       "      <td>[0.19546174, 0.29309717, 0.04253876, 0.0373506...</td>\n",
       "      <td>field</td>\n",
       "      <td>[1, 30, 19, 220, 5, 99, 222, 1189, 30, 373, 27...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2237</td>\n",
       "      <td>2896483502.jpg</td>\n",
       "      <td>[0.19546174, 0.29309717, 0.04253876, 0.0373506...</td>\n",
       "      <td>endseq</td>\n",
       "      <td>[1, 30, 19, 220, 5, 99, 222, 1189, 30, 373, 27...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2238 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          image_name                                              image  \\\n",
       "0     3275748918.jpg  [0.109782934, 0.30716428, 0.4646111, 0.5252362...   \n",
       "1     3275748918.jpg  [0.109782934, 0.30716428, 0.4646111, 0.5252362...   \n",
       "2     3275748918.jpg  [0.109782934, 0.30716428, 0.4646111, 0.5252362...   \n",
       "3     3275748918.jpg  [0.109782934, 0.30716428, 0.4646111, 0.5252362...   \n",
       "4     3275748918.jpg  [0.109782934, 0.30716428, 0.4646111, 0.5252362...   \n",
       "...              ...                                                ...   \n",
       "2233  2896483502.jpg  [0.19546174, 0.29309717, 0.04253876, 0.0373506...   \n",
       "2234  2896483502.jpg  [0.19546174, 0.29309717, 0.04253876, 0.0373506...   \n",
       "2235  2896483502.jpg  [0.19546174, 0.29309717, 0.04253876, 0.0373506...   \n",
       "2236  2896483502.jpg  [0.19546174, 0.29309717, 0.04253876, 0.0373506...   \n",
       "2237  2896483502.jpg  [0.19546174, 0.29309717, 0.04253876, 0.0373506...   \n",
       "\n",
       "      comment2                                    input_sequences  \\\n",
       "0     startseq  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1            a  [1, 30, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "2        young  [1, 30, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "3          boy  [1, 30, 3, 359, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4      dressed  [1, 30, 3, 359, 165, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "...        ...                                                ...   \n",
       "2233   frisbee  [1, 30, 19, 220, 5, 99, 222, 1189, 30, 373, 27...   \n",
       "2234        in  [1, 30, 19, 220, 5, 99, 222, 1189, 30, 373, 27...   \n",
       "2235         a  [1, 30, 19, 220, 5, 99, 222, 1189, 30, 373, 27...   \n",
       "2236     field  [1, 30, 19, 220, 5, 99, 222, 1189, 30, 373, 27...   \n",
       "2237    endseq  [1, 30, 19, 220, 5, 99, 222, 1189, 30, 373, 27...   \n",
       "\n",
       "                                       output_sequences  \n",
       "0     [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2     [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "...                                                 ...  \n",
       "2233  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2234  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2235  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2236  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2237  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "\n",
       "[2238 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "validation_data = list(load_batch_of_images(df_test, batch_size=len(df_test), \n",
    "                                                 model_for_encoding=model_new, \n",
    "                                                 folder_imgs_path=folder_proj_path+flickr_folder, \n",
    "                                                                    epochs=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "159 photos and input sequences processed in 58 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[ input1, input2], output] = validation_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2356, 2048), (2356, 78), (2356, 5464))"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input1_val.shape, input2_val.shape, output_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "steps = len(df)//batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks and model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import LambdaCallback, EarlyStopping, ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('RNN_model.sav', monitor='val_loss', period=2, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_losses_after_each_batch = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NbCallFunction:\n",
    "    \"\"\" This is a decorator to count the number of times a function has been called\n",
    "    It will be used to retrieve the image number to put it in working directory\"\"\"\n",
    "    def __init__(self, function):\n",
    "        self.callNumber = 0\n",
    "        self.function = function\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        ## onCall\n",
    "        self.callNumber += 1\n",
    "        return self.function(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "@NbCallFunction\n",
    "def compute_val_loss(batch, logs={}):\n",
    "    ## every 5 call \n",
    "    if compute_val_loss.callNumber  % 5 == 0:\n",
    "        val_loss = model.evaluate(*validation_data[0])\n",
    "        val_losses_after_each_batch[compute_val_loss.callNumber] = val_loss\n",
    "        print(\"training_loss: {} | val_loss: {}\".format(logs.get('loss'), val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda1 = LambdaCallback(on_batch_begin=compute_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20\n",
      "\n",
      "batch number : 1 | batch_size : 32\n",
      "\n",
      "starting sequence convertion to index based on entire dataset vocabulary\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(generator=load_batch_of_images(\n",
    "                            df, batch_size=32, \n",
    "                            model_for_encoding=model_new, \n",
    "                            folder_imgs_path=folder_proj_path+flickr_folder), \n",
    "                    steps_per_epoch=steps, initial_epoch=1,\n",
    "                    epochs=epochs,\n",
    "                    validation_data = validation_data[0], callbacks=[checkpoint, lambda1],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2238/2238 [==============================] - 3s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.231291672925634"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(*validation_data[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
