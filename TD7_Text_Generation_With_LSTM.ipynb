{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import github_command as gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.push(file_to_transfer=\"TD7_Text_Generation_With_LSTM.ipynb\",\n",
    "       message=\"text generation\",\n",
    "       repos=\"TDs_ESILV.git\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/lucbertin/anaconda3/lib/python3.6/site-packages (3.3)\r\n",
      "Requirement already satisfied: six in /Users/lucbertin/anaconda3/lib/python3.6/site-packages (from nltk) (1.11.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.text import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/lucbertin/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice = nltk.corpus.gutenberg.words('carroll-alice.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters: 150118\n",
      "Total Vocab: 46\n",
      "Total Patterns: 150018\n"
     ]
    }
   ],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "#filename = \"wonderland.txt\"\n",
    "#raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "#raw_text = raw_text.lower()\n",
    "raw_text = \" \".join(alice).lower()\n",
    "\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: {}\".format(n_chars))\n",
    "print(\"Total Vocab: {}\".format(n_vocab))\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    #print(seq_in)\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    #print(seq_out)\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    #print(dataX)\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "    #print(dataY)\n",
    "    \n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: {}\".format(n_patterns))\n",
    "\n",
    "### \n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150018, 100, 1), (150018, 46))"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 256)               264192    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 46)                11822     \n",
      "=================================================================\n",
      "Total params: 276,014\n",
      "Trainable params: 276,014\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150018, 100, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "  5000/150018 [..............................] - ETA: 5:06 - loss: 2.8943"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-fcb084521a77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist = model.fit(X, y, epochs=1, batch_size=500, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" d concluded the banquet --] ' what is the use of repeating all that stuff ,' the mock turtle interru \"\n",
      " , ' io the toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe toe \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print( \"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" looking at everything about her , to pass away the time . alice had never been in a court of justice \"\n",
      " , ' io ' s '  said the cat . ' io ' s ' m ' t ' v aen ' t ' ve toe taad ,' shi hatter sard to herself , ' in ' s ' m ' t ' v aen ' t ' ve toe taad ,' said the cat . ' io ' s ' m ' t ' v aen ' t ' ve toe taad ,' shi hatter sard to herself , ' in ' s ' m ' t ' v aen ' t ' ve toe taad ,' said the cat . ' io ' s ' m ' t ' v aen ' t ' ve toe taad ,' shi hatter sard to herself , ' in ' s ' m ' t ' v aen ' t ' ve toe taad ,' said the cat . ' io ' s ' m ' t ' v aen ' t ' ve toe taad ,' shi hatter sard to herself , ' in ' s ' m ' t ' v aen ' t ' ve toe taad ,' said the cat . ' io ' s ' m ' t ' v aen ' t ' ve toe taad ,' shi hatter sard to herself , ' in ' s ' m ' t ' v aen ' t ' ve toe taad ,' said the cat . ' io ' s ' m ' t ' v aen ' t ' ve toe taad ,' shi hatter sard to herself , ' in ' s ' m ' t ' v aen ' t ' ve toe taad ,' said the cat . ' io ' s ' m ' t ' v aen ' t ' ve toe taad ,' shi hatter sard to herself , ' in ' s ' m ' t ' v aen ' t ' ve toe taad ,' said the cat . ' io ' s ' m ' t '\n",
      "Done.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_file = \"weights-improvement-01-2.4514.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(weights_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 256)               264192    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 46)                11822     \n",
      "=================================================================\n",
      "Total params: 276,014\n",
      "Trainable params: 276,014\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_softmax(preds):\n",
    "    import numpy as np\n",
    "    preds = np.reshape(preds, -1)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_from_softmax([0,0.2,0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" their slates , ' she doesn ' t believe there ' s an atom of meaning in it ,' but none of them attemp \"\n",
      " . ' to faereis oneec . shue , aed ni lameey qo .' shid  ltec : ' tfu d tael ? why tant ,l eami , ro ihdeh ''aatelts oe ,  navdri blice : ' teacr jo oeeciest ,  'reld the cflpesu tone woe maah a''-wo mnsteadtg teon !'aou vreoonet a autdenan \"' ihee thi ln lhuil ih opgx ti dnady . sou mtdoglt g '    wh d solh b oerr ihggy a- 'tdg mpeee , '   ' * * * ( * * * * * * * * *sd tout ih a'' niseee tfrlrsg c ''n souu ,'d noeh sflpeemy !  saad mhe goeghnt . ' ie   ie*tgia g'romeeec ch soon , t ! ahd ,  'abnd iiot afdeg io sou tey loonynl lone po hypt ; iheeee as cuos diacp soeh b'mifdcgu ohcsendl lo sopcdoo fhhtel ieitha oaop lo a ' ih nlee veu --y thee rfe yoa wis ,tt io maa luueoa toiiee ! 'si , cutkehl , and the aioy soo mien , 'oeiwe mamd haemm bi mhet areihef dnrloude-c . ' neud wh  tand ,   ' '   b * * vhn , ilt go vhir vro ('r wai toan seoy ho tolyey wn sidt ,  she taoh t eryc.ahln the siyl doopr dyodige b ru yhs bet ; ihdc cetea '   anr soedlg auitedi '' ' ay woey ii reoe rasn the gidn ,'\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print( \"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    # take the sequence ( <=> pattern), reshape\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    # normalize\n",
    "    x = x / float(n_vocab)\n",
    "    # predict next character\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    # sample from softmax output for a little bit of variance\n",
    "    index = sample_from_softmax(prediction)\n",
    "    # transform index to char from dict\n",
    "    result = int_to_char[index]\n",
    "    # show back the pattern to the user\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    # entry sequence must have same lenght so drop the first character\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.19839218e-02, 2.41464382e-04, 3.33781245e-05, 7.52547130e-05,\n",
       "        6.78745637e-05, 1.55610578e-05, 1.09534645e-04, 4.16072464e-04,\n",
       "        1.63882403e-04, 4.79996816e-05, 2.81081475e-06, 4.95848099e-06,\n",
       "        4.67444715e-06, 3.57902263e-06, 2.63577749e-05, 2.62667854e-05,\n",
       "        3.35156328e-05, 1.49173457e-05, 9.46370164e-06, 1.11620529e-05,\n",
       "        4.68286984e-02, 3.21025820e-03, 3.85166146e-02, 8.32669660e-02,\n",
       "        2.32710198e-01, 8.15854128e-03, 1.31016383e-02, 4.32418324e-02,\n",
       "        3.38758342e-02, 1.89665428e-04, 7.29463622e-03, 1.36435134e-02,\n",
       "        9.28971730e-03, 5.13990596e-02, 1.91199742e-02, 1.37465633e-02,\n",
       "        2.27772645e-04, 2.84235794e-02, 3.73835564e-02, 1.27047941e-01,\n",
       "        2.95352563e-02, 4.50058654e-03, 1.32524297e-02, 6.22327439e-04,\n",
       "        4.79218252e-02, 1.98288282e-04]], dtype=float32)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23, 39,  0])"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2, 32, 23],\n",
       "       [ 2, 32, 39],\n",
       "       [ 2, 32,  0]])"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.tile([2,32], (3,1))\n",
    "np.c_[A, best_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" began bowing to the king , the queen , the royal children , and everybody else . ' leave off that !' \"\n",
      "(46,)\n",
      "[[24 26 20 33  0 21 34 42 28 33 26  0 39 34  0 39 27 24  0 30 28 33 26  0\n",
      "   7  0 39 27 24  0 36 40 24 24 33  0  7  0 39 27 24  0 37 34 44 20 31  0\n",
      "  22 27 28 31 23 37 24 33  0  7  0 20 33 23  0 24 41 24 37 44 21 34 23 44\n",
      "   0 24 31 38 24  0  9  0  3  0 31 24 20 41 24  0 34 25 25  0 39 27 20 39\n",
      "   0  1  3 38]\n",
      " [24 26 20 33  0 21 34 42 28 33 26  0 39 34  0 39 27 24  0 30 28 33 26  0\n",
      "   7  0 39 27 24  0 36 40 24 24 33  0  7  0 39 27 24  0 37 34 44 20 31  0\n",
      "  22 27 28 31 23 37 24 33  0  7  0 20 33 23  0 24 41 24 37 44 21 34 23 44\n",
      "   0 24 31 38 24  0  9  0  3  0 31 24 20 41 24  0 34 25 25  0 39 27 20 39\n",
      "   0  1  3 39]\n",
      " [24 26 20 33  0 21 34 42 28 33 26  0 39 34  0 39 27 24  0 30 28 33 26  0\n",
      "   7  0 39 27 24  0 36 40 24 24 33  0  7  0 39 27 24  0 37 34 44 20 31  0\n",
      "  22 27 28 31 23 37 24 33  0  7  0 20 33 23  0 24 41 24 37 44 21 34 23 44\n",
      "   0 24 31 38 24  0  9  0  3  0 31 24 20 41 24  0 34 25 25  0 39 27 20 39\n",
      "   0  1  3  0]]\n",
      "[[ -5.415473   -28.261118   -27.972147   -26.909262   -32.010204\n",
      "  -28.61218    -35.73461    -26.063429   -22.734915   -28.598494\n",
      "  -37.006798   -35.4553     -36.244816   -36.893337   -27.620214\n",
      "  -28.32213    -31.298082   -34.34086    -34.378593   -32.834606\n",
      "   -6.1943526  -17.26966    -14.839392   -11.412029    -0.5491959\n",
      "  -10.555077   -12.72005     -0.93177474  -5.475066   -26.307207\n",
      "  -15.5495615   -7.4103675  -14.182975    -6.436224    -4.456439\n",
      "  -10.179016   -24.444208    -7.285826   -10.088433    -6.9683104\n",
      "   -6.001675   -15.208559   -14.261864   -20.439869   -10.814409\n",
      "  -24.485094  ]\n",
      " [ -5.480238   -28.307161   -28.03758    -26.936941   -32.076477\n",
      "  -28.67373    -35.7773     -26.1094     -22.779263   -28.647036\n",
      "  -37.07205    -35.502323   -36.309082   -36.98397    -27.683405\n",
      "  -28.384932   -31.344534   -34.410637   -34.451836   -32.901016\n",
      "   -6.1881433  -17.304747   -14.87071    -11.427848    -0.55270576\n",
      "  -10.586475   -12.756941    -0.92572695  -5.481791   -26.352394\n",
      "  -15.5847225   -7.44586    -14.194067    -6.4428663   -4.452217\n",
      "  -10.187685   -24.499323    -7.300022   -10.130514    -6.990453\n",
      "   -6.024268   -15.223367   -14.2838745  -20.484415   -10.841846\n",
      "  -24.542425  ]\n",
      " [ -4.932815    -4.203012    -6.558008    -4.0774646   -7.8527436\n",
      "   -7.0552125   -9.515411    -2.5814176   -4.6820636   -3.81283\n",
      "  -10.13965     -9.484928    -9.649246    -9.500722    -4.697269\n",
      "   -5.6135526   -4.086145    -8.909428    -9.7532625   -8.708561\n",
      "   -2.4718442   -3.4351573   -3.820221    -4.131386    -3.4858916\n",
      "   -4.1879425   -4.9937453   -2.3643894   -3.0496202   -5.1400213\n",
      "   -3.6615362   -3.8204935   -2.880566    -3.25679     -3.4715571\n",
      "   -3.818726    -5.491713    -3.4510787   -2.5884476   -2.6754801\n",
      "   -4.6167936   -4.421121    -2.9574213   -7.8163347   -4.201947\n",
      "   -6.1310987 ]]\n",
      "[[-4.456439   -0.93177474 -0.5491959 ]\n",
      " [-4.452217   -0.92572695 -0.55270576]\n",
      " [-2.5814176  -2.4718442  -2.3643894 ]]\n",
      "[[34 27 24]\n",
      " [34 27 24]\n",
      " [ 7 20 27]]\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print( \"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "main_sentence = pattern\n",
    "k = 3\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    # take the sequence ( <=> pattern), reshape (batch=1, len seq, features)\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    # normalize\n",
    "    x = x / float(n_vocab)\n",
    "    \n",
    "    # predictions for next character\n",
    "    predictions = np.log(model.predict(x, verbose=0)).reshape(-1)\n",
    "    print(predictions.shape)\n",
    "    \n",
    "    # takes k best\n",
    "    best_k = np.argsort(predictions)[-k:]\n",
    "    # and their respective score\n",
    "    best_k_scores  = np.sort(predictions)[-k:]\n",
    "        \n",
    "    # k new sentences proposals  \n",
    "    proposals = np.tile(pattern, (3,1))\n",
    "    proposals = np.c_[proposals, best_k][:,1:]\n",
    "    print(proposals)\n",
    "    # predict all k sequences\n",
    "    predictions = np.log(model.predict(proposals.reshape(k, len(pattern), 1), batch_size=k))\n",
    "    # takes k best FOR EACH k SEQUENCE\n",
    "    print(predictions)\n",
    "    best_k_after = np.argsort(predictions)[:,-k:]\n",
    "    best_k_scores_after = np.sort(predictions)[:,-k:]\n",
    "    print(best_k_scores)\n",
    "    # multiply with previous scores: \n",
    "    best_k_after\n",
    "    \n",
    "    #print(predictions)\n",
    "    #best_k = [ np.argsort(prediction)[-k:] for prediction in predictions]\n",
    "    print(best_k)\n",
    "    break\n",
    "    # update predicted seq with highest score\n",
    "    #for seq, score in intermediate_results.items():\n",
    "        \n",
    "    # explore next iteration\n",
    "    #for i_, score in intermediate_results.items():\n",
    "        # show next proposals\n",
    "        ## could be optimized using as a batch\n",
    "    #print(proposals.shape)\n",
    "    #print(proposals)\n",
    "    #break\n",
    "    #index = sample_from_softmax(prediction)\n",
    "    # transform index to char from dict\n",
    "    \n",
    "    # show back the pattern to the user\n",
    "    #seq_in = [int_to_char[value] for value in pattern]\n",
    "    #sys.stdout.write(result)\n",
    "    #pattern.append(index)\n",
    "    # entry sequence must have same lenght so drop the first character\n",
    "    #pattern = pattern[1:len(pattern)]\n",
    "    \n",
    "    #main_sentence.append()\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decoder(sequence, k, model):\n",
    "    # probabilities\n",
    "    probs  = model.predict(x, verbose=0)\n",
    "    # select k best\n",
    "    best_k = np.argsort(probs)[-k:]\n",
    "    # append to original sequence as different proposal new sequences\n",
    "    proposals = [sequence + [elem] for elem in best_k]\n",
    "    # append \n",
    "    # \n",
    "    #\n",
    "    \n",
    "    sequences = [[list(), 0.0]]\n",
    "    # walk over each step in sequence\n",
    "    for row in data:\n",
    "        all_candidates = list()\n",
    "        # expand each current candidate\n",
    "        for i in range(len(sequences)):\n",
    "            seq, score = sequences[i]\n",
    "#             for j in range(len(row)): # instead of exploring all the labels, explore only k best at the current time\n",
    "            \n",
    "            # explore k best\n",
    "            for j in best_k:\n",
    "                candidate = [seq + [j], score + tf.math.log(row[j])]\n",
    "                all_candidates.append(candidate)\n",
    "        # order all candidates by score\n",
    "        ordered = sorted(all_candidates, key=lambda tup:tup[1], reverse=True)\n",
    "        # select k best\n",
    "        sequences = ordered[:k]\n",
    "    return sequences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
